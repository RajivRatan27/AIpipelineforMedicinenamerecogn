# -*- coding: utf-8 -*-
"""distilbertdatasettraining

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wt3d-wINeuBC8CDQlP9MhqhT_K9V7p2z
"""

!pip install pandas
!pip install regex

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import re

df = pd.read_csv("/content/drive/MyDrive/dataset1.csv")
medicine_terms = ['tablet', 'syrup', 'inhaler', 'capsule', 'injection', 'ointment', 'solution']

def clean_text(text):
    if pd.isna(text):  # Check for NaN values
        return text
    # Remove numbers, content within brackets, and brackets
    text = re.sub(r'\d+', '', text)         # Remove numbers
    text = re.sub(r'\(.*?\)', '', text)      # Remove text within brackets and brackets themselves
    # Remove medicine-related terms
    pattern = r'\b(?:' + '|'.join(medicine_terms) + r')\b'
    text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Apply the cleaning function to each relevant column
df['name'] = df['name'].apply(clean_text)
df['short_composition1'] = df['short_composition1'].apply(clean_text)
df['short_composition2'] = df['short_composition2'].apply(clean_text)

# Save the cleaned dataset
df.to_csv("cleaned_dataset.csv", index=False)  # Save the cleaned dataset to a new file
print("Dataset cleaned and saved as 'cleaned_dataset.csv'")

df = pd.read_csv("/content/cleaned_dataset.csv")
df['combined'] = df['name'] + ' ' + df['short_composition1']

# Create a new DataFrame with only the combined column
df_combined = df[['combined']]

# Save the new dataset with only the combined column
df_combined.to_csv("combined_dataset.csv", index=False)
print("Combined dataset saved as 'combined_dataset.csv'")

import pandas as pd
import re

# Load the dataset
df = pd.read_csv("combined_dataset.csv")  # Replace with your dataset path

# List of composition-related terms to remove
composition_terms = ['mg', 'ml', 'mcg', 'g', 'gram', 'grams', 'litre', 'liter', 'kg', 'milliliter', 'microgram']

# Function to clean the 'combined' column
def clean_composition_words(text):
    if pd.isna(text):  # Check for NaN values
        return text
    # Remove composition-related terms
    pattern = r'\b(?:' + '|'.join(composition_terms) + r')\b'
    text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    # Remove full stops and slashes
    text = re.sub(r'[./%]', '', text)
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Apply the cleaning function to the 'combined' column
df['combined'] = df['combined'].apply(clean_composition_words)

# Save the updated dataset
df.to_csv("cleaned_combined_dataset.csv", index=False)
print("Updated dataset saved as 'cleaned_combined_dataset.csv'")

import pandas as pd

# Load the cleaned dataset
df = pd.read_csv("cleaned_combined_dataset.csv")  # Replace with your dataset path

# Function to generate partial names by taking the first 40% of the original name
def create_partial_name(full_name):
    if pd.isna(full_name):  # Check for NaN values
        return full_name
    # Calculate the length of the name
    length = len(full_name)
    # Calculate 40% of the length
    partial_length = int(length * 0.4)
    # Return the substring that makes up 40% of the full name
    return full_name[:partial_length]

# Create partial names
df['partial_name'] = df['combined'].apply(create_partial_name)

# Keep only relevant columns
df['full_name'] = df['combined']  # Keep the full name as is
df = df[['partial_name', 'full_name']]

# Save this prepared dataset
df.to_csv("distillbert_training_dataset.csv", index=False)
print("Dataset saved as 'distillbert_training_dataset.csv'")

!pip install torch transformers datasets

import os
os.environ["WANDB_DISABLED"] = "true"

# Step 2: Import Libraries
import pandas as pd
import torch
from transformers import DistilBertTokenizer, DistilBertForMaskedLM, Trainer, TrainingArguments
from datasets import Dataset, DatasetDict

# Step 3: Load Dataset
data = pd.read_csv("/content/drive/MyDrive/distillbert_training_dataset.csv")  # Adjust path to your dataset

# Step 4: Preprocess and Prepare Dataset
data = data.rename(columns={"partial_name": "input_text", "full_name": "target_text"})
dataset = Dataset.from_pandas(data)
train_test_valid = dataset.train_test_split(test_size=0.3)
test_valid = train_test_valid['test'].train_test_split(test_size=0.33)
datasets = DatasetDict({
    'train': train_test_valid['train'],
    'test': test_valid['test'],
    'validation': test_valid['train']
})

# Step 5: Tokenize
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForMaskedLM.from_pretrained("distilbert-base-uncased")

def tokenize_function(examples):
    inputs = tokenizer(examples["input_text"], padding="max_length", truncation=True, max_length=64)
    targets = tokenizer(examples["target_text"], padding="max_length", truncation=True, max_length=64)
    inputs["labels"] = targets["input_ids"]
    return inputs

tokenized_datasets = datasets.map(tokenize_function, batched=True, remove_columns=["input_text", "target_text"])

# Step 6: Training Arguments and Trainer
training_args = TrainingArguments(
    output_dir="./distilbert_name_prediction",
    run_name="distilbert_name_prediction_run",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
)


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
)

# Step 7: Train and Save
trainer.train()
model.save_pretrained("/content/drive/MyDrive/distilbert_name_prediction")
tokenizer.save_pretrained("/content/drive/MyDrive/distilbert_name_prediction")
print("Model training complete and saved!")